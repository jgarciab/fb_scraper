{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datascraping Advanced Issues\n",
    "Dec 10th, 2018 - Javier Garcia-Bernardo, Anna Keuchenius & Allie Morgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Hidden\" APIs\n",
    "\n",
    "First, let's try and access what we are calling a \"hidden\" API. That is, we investigate the resources requested by a webpage (e.g. a list of faculty), and make requests directly to that API. \n",
    "\n",
    "We will do this for the website: https://www.uvm.edu/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, vist uvm.edu/directory and open the network tab as you do a search in this directory.  \n",
    "Copy the get url, and paste it on the website https://curl.trillworks.com/, that will convert directly to a python requests command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_names(letters):\n",
    "    params = (\n",
    "        ('name', letters),\n",
    "        ('request_num', '1'),\n",
    "    )\n",
    "\n",
    "    response = requests.get('https://www.uvm.edu/directory/api/query_results.php', params=params)\n",
    "    if response.ok == True:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = get_names(\"john smith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_json = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affiliate jfsmith@uvm.edu John F. Smith\n",
      "Student dsmith41@uvm.edu David John Smith\n",
      "Affiliate jfsmith@uvm.edu John F. Smith\n",
      "Student dsmith41@uvm.edu David John Smith\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display\n",
    "for i, person in enumerate(response_json[\"data\"]):\n",
    "#     display(person)\n",
    "    if i == 10: \n",
    "        break # Make sure we don't print too much\n",
    "        \n",
    "    print(person[\"edupersonprimaryaffiliation\"][\"0\"], person[\"edupersonprincipalname\"][\"0\"], person[\"cn\"][\"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session ID's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : Web of Science / Webofknowledge (only works if uvavpn is on, or connected to eduroam)\n",
    "- Beware, this website is hard to scrape,because it uses lots of params, cookies and data values\n",
    "- Some of these are not required, others are required (below we use the full list)\n",
    "- Next to the SID, there are some other variables that will be used if you scrape this website beyond the search page (such as qid and parentQid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 as bs\n",
    "import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "<!DOCTYPE html>                                                                                                                                                                                                                                                                                                                                             <html lang=\"en\">                                                                                                          <head><link rel=\"icon\" href=\"http://images.webofknowledge.com/WOKRS531NR4/images/wok_favicon.ico\" type=\"image/x-icon\"/> <title>Web of Science [v.5.31]  -     Web of Science Core Collection Basic Search  </title><link rel=\"stylesheet\" href=\"http://images.webofknowledge.com/WOKRS531NR4/css/WoKcommon.css\" type=\"text/css\" /><link rel=\"stylesheet\" href=\"http://images.webofknowledge.com/WOKRS531NR4/css/WoKcomponents.css\" type=\"text/css\" /><link rel=\"stylesheet\" href=\"http://images.webofknowledge.com/WOKRS531NR4/css/Font-SourceSans\n"
     ]
    }
   ],
   "source": [
    "print(\"Be patient, wos is a little slow\")\n",
    "page = requests.get('http://apps.webofknowledge.com')\n",
    "print(page.ok)\n",
    "print(page.is_redirect)\n",
    "print(page.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_sid(page):\n",
    "    # Parse html to soup element     \n",
    "    soup = bs.BeautifulSoup(page.text, 'html.parser')\n",
    "    # Find all urls on this page\n",
    "    link_elements = soup.find_all('a', href=True)\n",
    "    links = [a['href'] for a in link_elements]\n",
    "    # Find a url that mentions the SID\n",
    "    for link in links:\n",
    "        if \"&SID\" in link:\n",
    "            m = re.search('SID=([a-zA-Z0-9]*)&', link)\n",
    "            if m:\n",
    "                sid = m.group(1)\n",
    "                print(\"SID = \" + sid )\n",
    "                break\n",
    "    return sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SID = C6qUgZxsBpEFX5sdEyh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'C6qUgZxsBpEFX5sdEyh'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sid(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your searchstring for webofknowledge here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_for = \"duncan watts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The values for cookies, headers, and data were abstracted by copying cURL of the postrequest\n",
    "# and converting via https://curl.trillworks.com/ \n",
    "\n",
    "cookies = {\n",
    "    'JSESSIONID': '99D741BD336ECEAE76CAD3B393B28DEC',\n",
    "    '_abck': '71DAE2633D154E88038BB9AB588774F65F65025C79420000DE1B115CFB966873~0~iE9/DcS3kdvjrrZVkrxrxf8XZwdEfBkj8wGcJpsxPgk=~-1~-1',\n",
    "    'bm_sz': '2486FC5ACB86312F6255A7068D5A0A8E~QAAQXAJlX9Dp8ptnAQAABd3UojnpkufjEno/Soup6ud0aOug8vqn2/Y30pnLBro8ZWGeym3eeUxQshGhvslq7JSMKrN8C+ID0Bs6zOPNSf4nQyWyEbNFB/tqDePD/S+DIMGDuzSA1Rw6qg9NWfPLxjUDZSz1w+HPiXDTmDjvdgtXeVmsGboBc4ki7nLbZ92MXa5X417dDw==',\n",
    "    'SID': sid,\n",
    "    'CUSTOMER': 'SURFMARKET BV_Netherlands Consortium',\n",
    "    'E_GROUP_NAME': 'Universiteit van Amsterdam',\n",
    "    'dotmatics.elementalKey': 'SLsLWlMhrHnTjDerSrlG',\n",
    "    'ak_bmsc': '3E7A47598CAAF5268D5B25BC910E3E4B5F65025C79420000DE1B115C0EE4B205~plMIIaKYNxqVa5r3ZwEYYpeARdwS1xfcg++jvnlggN+Bgd7CHMxWIY9Xlmm+OWlBrw918+OtL+aC5vf0p4TXjAagNBuPBjLZfu0eH4dd7u5ckNy5xdogi7r9pMRzyududsEufRAZeC7KgI3taS383LnuQVS/6DLROYU5Cnt3nHdUtUiKhJMg4YRjCqXclJvRt/Tsy4Cgx/YkTooXOkvv5bL+3tSqjLcW8ZsiF+lmZ5n1+xK/7LCXQQ30R56gyrLjZQ',\n",
    "    'bm_sv': 'FFAA3183B0316D39CBFC5803DA96F660~jilZYddUoiGVvK4MVDpSTAjE+YXfAiZnhnCH+bAGZMzN67D761Q3XiYXHoyB4wNiq/Twf2oDTf9Pg6kcUDM8v8ym7WSirBJu3jbW5Cw3C5GvHjGSthV7IfsxJ/7WEDEF+PeY22WlfjvM8zeiuhnQh4RdWkO5qrzEOwvbkXS9El4=',\n",
    "    '_sp_ses.630e': '*',\n",
    "    '_sp_id.630e': 'b85ae547-8640-4c9d-815d-4ac2d5004766.1544625121.1.1544625121.1544625121.6ebc7484-bc0e-4fbb-ad02-e771ba38fb56',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:63.0) Gecko/20100101 Firefox/63.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Referer': 'http://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=E1hJmPezAu8d99A5SoW&preferencesSaved=',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "}\n",
    "\n",
    "data = [\n",
    "  ('fieldCount', '1'),\n",
    "  ('action', 'search'),\n",
    "  ('product', 'WOS'),\n",
    "  ('search_mode', 'GeneralSearch'),\n",
    "  ('SID', sid),\n",
    "  ('max_field_count', '25'),\n",
    "  ('max_field_notice', 'Notice: You cannot add another field.'),\n",
    "  ('input_invalid_notice', 'Search Error: Please enter a search term.'),\n",
    "  ('exp_notice', 'Search Error: Patent search term could be found in more than one family (unique patent number required for Expand option) '),\n",
    "  ('input_invalid_notice_limits', ' <br/>Note: Fields displayed in scrolling boxes must be combined with at least one other search field.'),\n",
    "  ('sa_params', 'WOS||E1hJmPezAu8d99A5SoW|http://apps.webofknowledge.com|\\''),\n",
    "  ('formUpdated', 'true'),\n",
    "  ('value(input1)', search_for),\n",
    "  ('value(select1)', 'TS'),\n",
    "  ('value(hidInput1)', ''),\n",
    "  ('limitStatus', 'collapsed'),\n",
    "  ('ss_lemmatization', 'On'),\n",
    "  ('ss_spellchecking', 'Suggest'),\n",
    "  ('SinceLastVisit_UTC', ''),\n",
    "  ('SinceLastVisit_DATE', ''),\n",
    "  ('period', 'Range Selection'),\n",
    "  ('range', 'ALL'),\n",
    "  ('startYear', '1975'),\n",
    "  ('endYear', '2018'),\n",
    "  ('editions', 'SCI'),\n",
    "  ('editions', 'SSCI'),\n",
    "  ('editions', 'AHCI'),\n",
    "  ('editions', 'ESCI'),\n",
    "  ('update_back2search_link_param', 'yes'),\n",
    "  ('ssStatus', 'display:none'),\n",
    "  ('ss_showsuggestions', 'ON'),\n",
    "  ('ss_numDefaultGeneralSearchFields', '1'),\n",
    "  ('ss_query_language', ''),\n",
    "  ('rs_sort_by', 'PY.D;LD.D;SO.A;VL.D;PG.A;AU.A'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.post('http://apps.webofknowledge.com/WOS_GeneralSearch.do', headers=headers, cookies=cookies, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The last apothecary: Eric Knott (1896-1993) and 20th-century pharmacy in Scotland\n",
      "\n",
      "\n",
      "AN INTERVIEW WITH DUNCAN WATTS\n",
      "\n",
      "\n",
      "\n",
      "Extensible software for whole of society modeling: framework and preliminary results\n",
      "\n",
      "\n",
      "The Measure of All Things Finding Out That Something Doesn't Work Is the First Step Toward Learning What Does Work\n",
      "\n",
      "\n",
      "Optimization and evaluation of a semi-continuous solar dryer for cereals (Rice, etc)\n",
      "\n",
      "\n",
      "The HBR list\n",
      "\n",
      "\n",
      "Complex systems: Network thinking\n",
      "\n",
      "\n",
      "In vitro caries formation in primary tooth enamel - Role of argon laser irradiation and remineralizing solution treatment\n",
      "\n",
      "\n",
      "Effect of the photoperiod on bullfrog (Rana catesbeiana Shaw, 1802) tadpoles development\n",
      "\n",
      "\n",
      "Researching Sara Jeanette Duncan in the papers of A.P. Watt and Company\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the titles of the first 10 search results\n",
    "soup = bs.BeautifulSoup(response.text, 'html.parser')\n",
    "results = soup.find_all('div', {'class': 'search-results-content'})\n",
    "for result in results:\n",
    "    print(result.find('a', {'class' : 'smallV110 snowplow-full-record'}).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawlers \n",
    "scaling up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, scraping can be easily parallelized. Especially if you have several urls that need to be scraped independently. In case you do a search on website, and get many result pages, you can also parallelize your code; you can divide the work of scraping over several scrapers that all scrape several pages. However, then you need to put in place a way of tracking what has been scraped and what has not. Maybe some of you, us, have advice on how to do this? I personally use a tracking table in my database, that tracks the progress. \n",
    "\n",
    "Again, robustness is important: build your scrapers or crawlers in such a way that it is absolutely fine if a scraper dies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to parallelize scrapers, i.e. setup crawlers. One of the ways is to do this yourself, without an external service, by means of subprocess. Here is some simple code I wrote to do this. This spin_up_scrapers code spins up several scrapers, and check every x seconds if each scraper is still active. If one dies, another scraper is spin up.\n",
    "main.py is the scraper code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "\n",
    "nr_scrapers = 10\n",
    "nr_hours_scraping = 10\n",
    "\n",
    "def spin_up_scraper(nr_scrapers, nr_hours_scraping):\n",
    "    scraper_processes = []\n",
    "    for scraper_i in range(nr_scrapers):\n",
    "        p = subprocess.Popen(['python main.py'], shell=True,\n",
    "                                stdin=None, stdout=None, stderr=None, close_fds=True)\n",
    "\n",
    "        # Wait a few moments before starting the next scraper\n",
    "        time.sleep(20)\n",
    "        print(\"---------------Starting next scraper-------------------------------\")\n",
    "        scraper_processes.append(p)\n",
    "\n",
    "\n",
    "    # Check every minute if all scrapers are up, if one is down, start a new one\n",
    "    for minutes in range(60*nr_hours_scraping):\n",
    "        # Sleep 60 seconds till the next check\n",
    "        time.sleep(60)\n",
    "        for scraper in scraper_processes:\n",
    "            down = scraper.poll()\n",
    "            if down is None:\n",
    "                scraper_processes.remove(scraper)\n",
    "                print('----One scraper down. Starting a new one ------------------------')\n",
    "                p = subprocess.Popen(['python main.py'], shell=True,\n",
    "                                     stdin=None, stdout=None, stderr=None, close_fds=True)\n",
    "                scraper_processes.append(p)\n",
    "                time.sleep(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrolling down\n",
    "Some websites (e.g. reddit or facebook) have infinite scroll. You only load new elements by scrolling down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium.webdriver\n",
    "driver = selenium.webdriver.Chrome()\n",
    "driver.get('https://cssamsterdam.github.io/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scroll_down(SCROLL_PAUSE_TIME = 0.5):\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height: break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scroll_down()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stale Element Exception\n",
    "We get this when the element is destroyed or hasn't been completely loaded. Possible solutions: Refresh the website, or wait until the page loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium.common.exceptions\n",
    "import selenium.webdriver\n",
    "import selenium.webdriver.common.desired_capabilities\n",
    "import selenium.webdriver.support.ui\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "#Define a function to wait for an element to load\n",
    "def _wait_for_element(xpath, wait):\n",
    "    try:\n",
    "        polling_f = expected_conditions.presence_of_element_located((selenium.webdriver.common.by.By.XPATH, xpath))\n",
    "        elem = wait.until(polling_f)\n",
    "    except:\n",
    "        raise selenium.common.exceptions.TimeoutException(msg='XPath \"{}\" presence wait timeout.'.format(xpath))\n",
    "    return elem\n",
    "\n",
    "def _wait_for_element_click(xpath, wait):\n",
    "    try:\n",
    "        polling_f = expected_conditions.element_to_be_clickable((selenium.webdriver.common.by.By.XPATH, xpath))\n",
    "        elem = wait.until(polling_f)\n",
    "    except:\n",
    "        raise selenium.common.exceptions.TimeoutException(msg='XPath \"{}\" presence wait timeout.'.format(xpath))\n",
    "    return elem\n",
    "\n",
    "#define short and long timeouts\n",
    "wait_timeouts=(30, 180)\n",
    "\n",
    "#open the driver (change the executable path to geckodriver_mac or geckodriver.exe)\n",
    "driver = selenium.webdriver.Firefox(executable_path=\"./geckodriver\")\n",
    "\n",
    "#define short and long waits (for the times you have to wait for the page to load)\n",
    "short_wait = selenium.webdriver.support.ui.WebDriverWait(driver, wait_timeouts[0], poll_frequency=0.05)\n",
    "long_wait = selenium.webdriver.support.ui.WebDriverWait(driver, wait_timeouts[1], poll_frequency=1)\n",
    "\n",
    "\n",
    "#And this is how you get an element\n",
    "element = _wait_for_element('HERE_GOES_THE_XPATH',short_wait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pdfs\n",
    "Many times you want to download pdfs (applications/pdf) automatically (or some other file, e.g. for zip files \"application/x-gzip\"), but you get the pop ups asking you where to save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import selenium.webdriver\n",
    "\n",
    "##Chrome option\n",
    "options = selenium.webdriver.ChromeOptions()\n",
    "profile = {\"plugins.plugins_list\":\n",
    "           [{\"enabled\": False, \"name\": \"Chrome PDF Viewer\"}], # Disable Chrome's PDF Viewer\n",
    "           \"download.default_directory\": \"./download_directory/\" ,\n",
    "           \"download.extensions_to_open\": \"applications/pdf\"}\n",
    "options.add_experimental_option(\"prefs\", profile)\n",
    "   \n",
    "driver = selenium.webdriver.Chrome(\"./chromedriver\",chrome_options=options)\n",
    "\n",
    "\n",
    "# ##Firefox option\n",
    "# profile = selenium.webdriver.FirefoxProfile()\n",
    "# profile.set_preference(\"browser.download.folderList\", 2)\n",
    "# profile.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "# profile.set_preference(\"browser.download.dir\", \"./download_directory/\")\n",
    "# profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\")\n",
    "\n",
    "# driver = selenium.webdriver.Firefox(firefox_profile=profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headless Chrome\n",
    "Both Chrome and Firefox allow you to start headless (without opening a window)\n",
    "\n",
    "    import selenium.webdriver\n",
    "    options = selenium.webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = selenium.webdriver.Chrome(\"./chromedriver\",chrome_options=options)\n",
    "\n",
    "    \n",
    "    from selenium.webdriver.firefox.options import Options\n",
    "    options = Options()\n",
    "    driver = selenium.webdriver.Firefox(\"./geckodriver\",options=options)\n",
    "\n",
    "This is how you enable download in headless Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import selenium.webdriver\n",
    "\n",
    "def enable_download_in_headless_chrome(driver, download_dir):\n",
    "    # add missing support for chrome \"send_command\"  to selenium webdriver\n",
    "    driver.command_executor._commands[\"send_command\"] = (\"POST\", '/session/$sessionId/chromium/send_command')\n",
    "\n",
    "    params = {'cmd': 'Page.setDownloadBehavior', 'params': {'behavior': 'allow', 'downloadPath': download_dir}}\n",
    "    command_result = driver.execute(\"send_command\", params)\n",
    "\n",
    "options = selenium.webdriver.ChromeOptions()\n",
    "profile = {\"plugins.plugins_list\":\n",
    "           [{\"enabled\": False, \"name\": \"Chrome PDF Viewer\"}], # Disable Chrome's PDF Viewer\n",
    "           \"download.default_directory\": \"./download_directory/\" ,\n",
    "           \"download.extensions_to_open\": \"applications/pdf\"}\n",
    "\n",
    "options.add_experimental_option(\"prefs\", profile)\n",
    "options.add_argument(\"--headless\")\n",
    "   \n",
    "driver = selenium.webdriver.Chrome(\"./chromedriver\",chrome_options=options)\n",
    "enable_download_in_headless_chrome(driver,\"./download_directory/\")\n",
    "                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with new windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Click somewhere\n",
    "driver.find_element_by_xpath(\"xxxx\").click()\n",
    "\n",
    "#Switch to the new window\n",
    "driver.switch_to_window(driver.window_handles[1])\n",
    "\n",
    "#Do whatever\n",
    "driver.find_element_by_xpath('xxxxxx').click()\n",
    "\n",
    "#Go back to the main window\n",
    "driver.switch_to_window(driver.window_handles[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites changes their html all the time. Therefore it is worthwhile to make your scraper robust.  There are a few tips we have on how to do this, and you might know some other tricks too.\n",
    "\n",
    "- Don't make your scraper language dependent. Your browser setting will influence the text displayed on websites. So if you extracting elements by text, this is sensitive your browser setting, and to the general language of the website. It's better not extract elements by text.\n",
    "- Save raw html. As we learned last week from Damian Trillings Database Management workshop, it is very good practice to save the entire html of the website in stead of only the elements you are interested in. That way, if the website has changed their html and your scraper brakes or is not extracing the right elements anymore, you can simply re-extract the information later from the raw html that you saved in your database.\n",
    "- Use drilldown method. Don't look for a class or attribute in the entire html, but first drill down the specific part of page. Very bad practice is to look for all elements of very general html attribute (such as 'row'), which returns a list, and then select the right index of that list.  This is very sensitive to html changes!\n",
    "- I always try to avoid xpath, for the same reason as above.\n",
    "- Track your progress. Build you scraper in a way so that it is not a problem if it crashes. Scrapers will always crash, almost by default. For example, your vpn connenction can shut down (if applicable), your internet connection might brake, the site your scraping might go down for a moment etc. Track your progress somewhere so that you can always turn your scraper on so that it start from where it left of.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proxies\n",
    "- Proxies allow you to download a website changing your IP. \n",
    "**Don't use it for evil**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using public proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install http-request-randomizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from http_request_randomizer.requests.proxy.requestProxy import RequestProxy\n",
    "import logging\n",
    "\n",
    "# Collects the proxys and log errors\n",
    "req_proxy = RequestProxy()\n",
    "req_proxy.set_logger_level(logging.CRITICAL)\n",
    "\n",
    "# Request a website\n",
    "r = req_proxy.generate_proxied_request(\"http://cssamsterdam.github.io\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TOR\n",
    "- Instructions to configure it: https://github.com/jgarciab/tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This won't work without configuring it\n",
    "from tor_control import TorControl\n",
    "import requests\n",
    "\n",
    "\n",
    "tc = TorControl()\n",
    "print(requests.get(\"https://api.ipify.org?format=jso\").text)\n",
    "#> 163.172.162.106\n",
    "\n",
    "tc.renew_tor()\n",
    "print(requests.get(\"https://api.ipify.org?format=jso\").text)\n",
    "#> 18.85.22.204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed up requests \n",
    "- Use: Want to collect info from many different websites.\n",
    "- Problem: requests is blocking (it waits until the website responds )\n",
    "- Solution: run many threads\n",
    "  - But not straightforward\n",
    "  - Best: grequests: asynchronous HTTP Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grequests in /home/javiergb/Programs/anaconda3/lib/python3.5/site-packages (0.3.0)\r\n",
      "Requirement already satisfied: requests in /home/javiergb/Programs/anaconda3/lib/python3.5/site-packages (from grequests) (2.13.0)\r\n",
      "Requirement already satisfied: gevent in /home/javiergb/Programs/anaconda3/lib/python3.5/site-packages (from grequests) (1.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install grequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import grequests\n",
    "\n",
    "urls = [\n",
    "    'http://www.heroku.com','http://python-tablib.org', 'http://httpbin.org', \n",
    "    'http://python-requests.org', 'http://fakedomain/','http://kennethreitz.com'\n",
    "]\n",
    "\n",
    "rs = (grequests.get(u) for u in urls)\n",
    "\n",
    "grequests.map(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
